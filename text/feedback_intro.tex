The feedback problem is one of requesting more evidence for a diagnostic model to make a prediction. In the context of radiology reporting, evidence is the radiologist observations and annotations they make about an image. Because the universe of possible observations is large, it would be prohibitively expensive to request all such observations. Instead, the requesting system must ask as little information as possible to achieve high accuracy diagnosis.

Though there are many methods to acquire expensive features in partially observed classification scenarios, there has not been a serious effort to evaluate and compare them. Previous work has generally taken the approach of defining criteria to stop requesting more information, and then deriving a strategy to select the next piece of information that will most rapidly reach this stop criteria \cite{Heckerman:1992uq,Chi:tp,SaarTsechansky:2009bu}. Though this is a pragmatic approach, it makes the assumption that the (selection, stop) criteria pair will directly improve diagnosis. There has not been a systematic effort to show that these models actually directly improve classification accuracy. Moreover, there has been no work to decouple the (selection,stop) criteria pair to mix and match strategies. 

In this work, we propose that a feedback system is defined by two methods: selection criteria and stop criteria. Then we propose an evaluation framework to measure these pairs as a direct result on classification accuracy.