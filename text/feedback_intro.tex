Interpretation of mammograms is a difficult task subject to considerable variability in diagnostic performance \cite{Elmore:2009vu,Elmore:2012er,Berg:2002fy,Beam:1996ui,Barlow:2004cy}. Error stems from three main sources: poor image quality masking abnormalities, radiologist error in detection of abnormalities, and incorrect interpretation of abnormalities identified in the image. Detection and interpretation stem direction from the radiologist whereas image quality is dependent on referring physicians, radiography technicians, and equipment. Computer-aided systems to assist in the detection and interpretation of radiological images show promising results in academic and commerical applications, but they only focus on the data in the image. A promising area to provide decision support in interpretation is at the report level, where such systems show strong diagnostic performance \cite{Burnside:2000wl,ElizabethS:2005gc,Burnside:2009br}. The drawback of such systems is their dependence on information provided by the radiologist rather than simply relying upon the existence of image data. As a result, incomplete reports might not contain sufficient evidence to make accurate recommendations to the practicing radiologist. In this study, we propose a framewokr that allows decision support systems to elicit feedback from radiologists. 

The feedback problem is one of requesting more evidence for a diagnostic model to make a prediction. In the context of radiology reporting, evidence is the radiologist observations and annotations they make about an image. Because the universe of possible observations is large, it would be prohibitively expensive to request all such observations. Instead, the requesting system must ask as little information as possible to achieve high accuracy diagnosis.

Though there are many methods to acquire expensive features in partially observed classification scenarios, there has not been a serious effort to evaluate and compare them. Previous work has generally taken the approach of defining criteria to stop requesting more information, and then deriving a strategy to select the next piece of information that will most rapidly reach this stop criteria \cite{Heckerman:1992uq,Chi:tp,SaarTsechansky:2009bu}. Though this is a pragmatic approach, it makes the assumption that the (selection, stop) criteria pair will directly improve diagnosis. There has not been a systematic effort to show that these models actually directly improve classification accuracy. Moreover, there has been no work to decouple the (selection,stop) criteria pair to mix and match strategies. 

In this work, we propose that a feedback system is defined by two methods: selection criteria and stop criteria. Then we propose an evaluation framework to measure these pairs as a direct result on classification accuracy.