\prefacesection{Abstract}
Radiology is a powerful tool to detect and diagnose abnormalities by allowing doctors to visually inspect internal pathology that could not otherwise be seen. 
However, assessing radiological images is limited by variations among practitioners, including deficiencies in their reporting of these imaging examinations as well as in their interpretations. 
Three main sources of these variations in interpretation are incorrectness of observations in the images, incompleteness of the radiological observations reported to characterize the abnormalities, and inconsistency of these observations with respect to the radiologists' overall impression.
I hypothesized that delivering decision support during reporting time to improve completeness and correctness of reports would improve consistency, and therefore, diagnostic performance
To test this hypothesis, I formulated a decision support framework that provides feedback to radiologists during the reporting of their radiological observations.
I developed this system by creating novel statistical models to link radiological observations, computational imaging features, and disease to recognize incorrectness, incompleteness and inconsistency in reporting. 
I then harnessed these models to create a quantifiable metric of observation quality. 
In this dissertation, I describe this system with the following specific aims: (1) developed methods to assess completeness and correctness of radiology reports, (2) evaluated these methods in two important radiological domains (mammography and liver CT), and (3) developed framework to provide feedback to radiologist to ensure consistency between report and diagnosis, improving diagnostic performance.
I performed three major experiments to verify these aims.
In my first project, I developed an image annotation classifier that predicts the descriptors a radiologist would use in a report to describe a liver lesion on a CT scan. 
I predicted 30 different types of descriptors and had a mean AUC of $0.816\pm0.141$ with a misclassification rate of $0.1443\pm0.0881$. 
These results showed that the image annotation framework could be used as a second reader for radiological reports.
In my next project, I developed a novel metric to measure whether reports had enough information to justify the radiologist's diagnosis.
I found that this measure could accurately predict when radiologists make errors based solely on the evidence they give to justify their diagnosis, with $82.6\%$ classification accuracy.
Finally, I created a framework to deliver feedback to the radiologist in order to complete their report in the most efficient manner.
I found that using the aforementioned incompleteness score coupled with a myopic, mutual information descriptor selection criteria allows a decision support system to achieve $93.6\%$ diagnostic classification accuracy with an average of $4$ observations.
This achieves better classification accuracy than the decision support system that uses all 20 descriptors by $1.5\%$.
The results from these three experiments showed that it is feasible to deliver decision support to improve reports, \emph{and} that improving reports improves diagnostic performance.