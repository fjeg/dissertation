Decision support for radiologists can come in several forms based on the decision being made. Prevalent decision support methods involves the synthesis and retrieval of large data sets for reference, computer-aided detection of findings in images (CADe), computer-aided diagnosis of findings in images (CADx), and computer-aided recommendations of clinical management. I provide a survey of some prominent methods and works here.

\subsection{Radiology reference and data mining}
One of the earliest forms of decision support allows radiologists to query reference material and images. Generation of imaging databases that are searchable is non-trivial, involving significant time cleaning and annotating data. Natural language processing and computational image analysis allows practitioners to significantly speed up such work  \cite{Depeursinge:2012ce, Bozkurt:2014jw,Nassif:2009du}. In modern data-rich environments this has morphed into querying similar images to the finding in question via content-based image retrieval (CBIR) \cite{Akgul:2011ey}. CBIR frameworks require methods to represent data in a meaningful way and methods to compute the similarity between data points via these representations. Representations of images typically involve computational feature extract to quantify intensity, texture, edge, and shape \cite{Strela:2002vq,Zhao:2005wb, Hong:2006ti,Manay:2006un,MRangayyan:2005td,Xu:2012bh}, and semantic annotations of images via descriptors from radlex \cite{Langlotz:2006jn}. Similarity metrics can come in the form of basic vector distances to sophisticated classification frameworks \cite{Akgul:2011ey}. Modern systems like the biomedical imaging metadata manager (BIMM) makes use of computational and semantic descriptors in conjunction with a logistic regression framework for similarity in liver lesions \cite{Korenblum:2011gx}. More novel methods make use of automatically learned representations from data via deep learning \cite{Shin:2015wl}. 

\subsection{Computer-aided detection in images}
CADe is used to automatically identify regions of interest to guide the radiologist after they have initially viewed the image without help. The goal is to reduce false negatives in detection of relevant findings. CADe has also been the most commercially successful form of radiological decision support, with several products on the market and prominent usage amongst practicing radiologists \cite{Castellino:2005ke}. Generally, CADe systems perform computational image analysis on the medical image to extract features characterizing local regions in the image. These characterizations are used to create a saliency map of the entire image. Then techniques such as non-maximal suppression identify single points of high interest and deliver the marking to the radiologists. These markings are carefully tuned to have high degrees of sensitivity while balancing false positive marks \cite{Oliver:2010fm}. Mammography has seen the widest adoption of these methods for detection of calcifications and masses \cite{Cheng:2003ig,Castellino:2005ke,Meeuwis:2010bv,Oliver:2010fm,Fenton:2011fw,Fenton:2012kz,Jamieson:2012hz,Gallas:2012eg,Giger:2013jb}.


\subsection{Computer-aided diagnosis in images}

%\cite{Jiang:1999fj,ElizabethS:2005gc,Gallas:2012eg,Bright:2012ga,Giger:2013jb,Depeursinge:2010jl,Fujita:2008it,Eadie:2011cv,Rubin:2005jg,Garg:2005cb,Elter:2009fv,Jamieson:2010vl,Jamieson:2010tt,Cheng:2003ig,Jiang:2001fy}
CADx problems are classification tasks to predict the diagnosis of a finding given its manifestation on the image. The methodology is similar to that of CADe with regard to computational image analysis and feature extraction \cite{Jiang:1999fj,Jiang:2001fy, Giger:2013jb, Eadie:2011cv}. The main differentiating factor is that CADx systems take a specific finding as input and output a categorical value (or distribution over several categorical values). These systems have shown to improve the diagnostic performance as well as reduce variability amongst radiologists \cite{Garg:2005cb} in certain well-deployed settings, but they can also show adverse effects upon performance due to clinical reliance and trust in these systems \cite{Eadie:2011cv,Giger:2013jb}. Figure \ref{fig:cadx_effects} shows the results of a review of 48 CADx studies.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{cadx_effects.pdf}
	\caption[Effect of CADx on practice over 48 different studies]{The change in (weighted) mean sensitivity, specificity and ln DOR of cancer diagnosis between radiologists working alone, and when using CADx; data displayed by diagnostic application. Overall results for CADe studies are also displayed. A positive change means that CAD improved sensitivity or specificity; a negative change means that radiologists produced better results without CAD. The number of studies contributing to each group is indicated. Figure and caption from \cite{Eadie:2011cv}.}
	\label{fig:cadx_effects}
\end{figure}

\subsection{Computer-aided recommendations of clinical management}
Even if radiologists had perfect information about the location of lesions and all of their associated descriptors, they still need to make decisions regarding proper patient care. Current practice models have radiologists use heuristics to make these decisions which allow for errors based on subjective assessment. Decision support systems that analyze the evidence in a quantitative and repeatable manner are obvious solutions to removing this subjectivity. Several studies into Bayesian networks for diagnosis of mammograms showed promising results in outperforming image-based CADx systems (AUC = 0.943) \cite{Burnside:2000wl,ElizabethS:2005gc,Rubin:2005jg}. The issue with expert systems is that they are costly to create and have poor calibration with respect to the probabilities they estimate. Instead, learning the structure and parameters of these networks from data improves calibration and diagnostic accuracy \cite{Burnside:2009br} and actually outperformed interpreting radiologists (AUC 0.960 vs 0.939, p=0.002).

\subsection{Difficulties deploying radiological decision support}
Studies into deployment of medical decision-support systems help us understand why decision support systems have not seen adoption in clinical practice. Most such systems follow the Greek Oracle model of decision-support: they simply give an answer to the diagnostic task rather then assisting the radiologist to improve their own decision \cite{Miller:1990wg, Friedman:2009dx, Bright:2012ga}. Additionally, such systems interrupt the traditional radiological workflow \cite{Morgan:2011ct}. 

\subsection{Promise of decision support}
Decision support systems could potentially diminish subjectivity in the interpretation of clinical information using quantitative methods and objective decision points. In addition, it is be possible to directly measure and tune performance in decision support systems; a task that is much more challenging in unassisted human readers. The difference between such systems and radiologists is that this tradeoff in decision support systems is explicitly set as the “operating point” (a particular value of sensitivity and the consequent specificity) on the receiver operating characteristic (ROC) curve of the system, which is used to assess and maximize the performance of decision support systems. In probabilistic systems, a threshold solely determines the operating point. This threshold can be interpreted as the minimum probability of cancer that a lesion must exhibit before it is deemed a positive finding (i.e. considered for biopsy). Most radiologists strive to have a fixed operating point, but given the qualitative nature of mammography interpretation, it is not possible for them to know their probabilistic threshold precisely. This makes it difficult to quantify if individual radiologists are practicing too conservatively by setting their threshold low to avoid missing cancers. While it is of utmost importance to detect cancer, an unnecessarily low threshold means more false positive detections. Even if a radiologist were deemed to be practicing too conservatively, it would be difficult to tune performance to practice less aversely.












