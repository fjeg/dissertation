We evaluated the performance of selection and stop criteria both independently and jointly.
Our results show that optimizing either function can provide significant gains in the accuracy of decision-support systems.
They also reveal that jointly optimizing these criteria (i.e. developing selection criteria specifically to minimize stop criteria) is not necessarily the best way to improve overall classification accuracy.

\subsection{Selection criteria and performance}
We compared three different types of selection criteria: Random, Maximize Mutual Information (abbreviated as \emph{mutual information}), and Minimize Cross-Entropy (abbreviated as \emph{optimal}).
We measured how each of these selection criteria affect the classifier estimates of probability of malignancy, stop criteria scores, binary classification performance, and ranking importance of BI-RADS descriptors.

All cases studied start with zero observed descriptors and finish with all 20 descriptors observed.
Classification accuracy of the system with zero descriptors is $2.49\%$, and reaches $92.06\%$ with all observed descriptors.
Accuracy with any number of features between zero and 20 is dependent on the selection/stop criterion
We compare accuracy using some intermediary stop criterion to the results achieved by using all descriptors, as that is the prior published performance of this decision support system \cite{Gimenez:2014tr,Burnside:2009br}.

\subsection{Effect of selection criteria on probability of malignancy}
\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{probability_path.pdf}
	\caption[Effect of selection criteria on probability of malignancy]{Effect of different selection criteria on the posterior probability of malignancy. The x-axis represents how many observations are requested of the radiologist, and the y-axis represents posterior probability of malignancy. Benign and malignant lesions are plotted separately to show how more information increases the probability of each. The probabilities achieve their asymptotic maximums at 0.6 due to the variability in descriptor usage to train these models. }
	\label{fig:feedback_mammo}
\end{figure}


Perfect selection criteria should select descriptors that best differentiate malignant and benign masses as quickly as possible.
The most obvious metric for this is the decision-support system's posterior probability of malignancy.
A perfect feedback system would maximize the separation between these two groups using as few features as possible.

We measure this value for every lesion that a radiologist evaluates at each iteration of feedback.
Figure \ref{fig:feedback_mammo} shows the mean posterior probability of malignancy stratified by lesion malignancy.
We see that all three selection criteria do well in driving these posterior probabilities in the correct directions.
All three do a good job of driving probability of benign masses to zero quickly.
For malignant masses, there is a striking difference the rates that each method reaches maximal probability of malignancy.
Random selection has a linear path towards the maximum probability while mutual information has an inverted exponential path.
Optimal selection reaches maximal probability the fastest and exceeds the posterior probability of malignancy given all features.

\subsection{Effect of selection criteria on stop criteria scores}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{score_path.pdf}
	\caption[Effect of selection criteria on stop criteria scores]{Effect of different selection criteria on stop criteria scores. The left plot shows the selection criteria's effect on the incompleteness score and the right graph shows their effect on the entropy. The x-axis represents how many observations requested of the radiologist, and the y-axis represents each of the stop criteria scores.}
	\label{fig:feedback_score_path}
\end{figure} 

All the stop criteria rules we evaluated were thresholded score functions; they involve driving a numeric measure of confidence in the diagnosis below a defined threshold.
As a result, selection criteria should seek to minimize these stop criteria scores in order to stop requesting redundant information.
Figure \ref{fig:feedback_score_path} shows the effect of the three different selection criteria on the two stop criteria by plotting the mean stop score as a function of number of descriptors selected.

\emph{Effect on incompleteness}:
While all three methods drive the incompleteness score down with a single request for the first observation, their behavior diverges quickly.
Random feedback requests descriptors that make the report \emph{less} complete as evidence by the incompleteness score growing after the first requests.
This is due to the random selection introducing noisy observations that increase uncertainty in the diagnosis.
Mutual information has a linear decrease in the incompleteness score as more features are requested, though the effect is small.
Optimal selection drops the incompleteness score drastically, showing more complete reports with only a subset of total descriptors.
This is driven by the optimal selection criteria driving probabilities towards 0 or 1.
The further this posterior probability is from the 2\% decision threshold, the less likely new descriptors can have a large enough effect on the probability to cross this decision boundary.

\emph{Effect on entropy}:
Entropy is a submodular function meaning its value always decreases \emph{in expectation} when given new information to the system.
This is described by the principle that ``\emph{information never hurts}'' \cite{Nemhauser:1978ck,Krause:2005tr}.
Such behavior is evident in random and mutual information selection, which show monotonic decreasing of entropy of classification.
Despite the similar behavior, the mean entropy using mutual information selection is always lower than random selection outside of the trivial cases using 0 and all 20 features.
Optimal feedback achieves entropy below the theoretical minimum due to the fact that it ``cheats'' (i.e. looks ahead) at descriptors which most minimize this entropy.
As a result, it violates the property of submodularity since we do not calculate the next feature in expectation.


 \subsection{Effect of selection criteria on classification performance}
 \begin{figure}[h]
 	\centering
 	\includegraphics[width=\linewidth]{perf_path.pdf}
 	\caption[Effect of selection criteria on classification performance]{Effect of different selection criteria on accuracy, sensitivity, and specificity of classification of lesion malignancy. The x-axis represents how many observations requested of the radiologist, and the y-axis represents performance from 0-1. For all measures, a larger performance value is better.}
 	\label{fig:feedback_performance}
 \end{figure}

The most important factor in selection criteria is how it affects the performance of the decision-support system in classifying malignancy of lesions, as measured by accuracy, sensitivity, and specificity.
The three different selection criteria are clearly differentiated, with "Optimal" as the best method, followed by "Mutual Information", and finally, "Random" [Figure \ref{fig:feedback_performance}].

\emph{Effect on accuracy}: 
Optimal selection achieves perfect classification accuracy by selecting a single descriptor.
It maintains perfect classification accuracy until it reaches 12 descriptors, when the inherent error of the model affects performance.
Mutual information selection has then second fastest improvement on accuracy.
It reaches the model's final accuracy value using only 6 descriptors (compared to all 20).
The increase in performance is monotonic, though there is a plateau in accuracy between 2 and 3 features, showing that there must be a minimum amount of information to cross this accuracy threshold.
Finally, random feature selection has the slowest increase in accuracy, though the performance gain still shows faster (in terms of number of descriptors required) than linear improvement.

\emph{Effect on sensitivity}:
In general, sensitivity is maximized when no features are selected since the model will initially predict that every mass is malignant.
Thus, every selection criteria will only reduce sensitivity, and the best behavior is to do so as slowly as possible.
Optimal feedback has perfect sensitivity until 12 descriptors are selected, and the model's inherent error drives this down for all remaining descriptors.
Mutual information drops sensitivity to the same performance as that of fully-observed reports within two descriptors, but never goes below this value.
Random selection, on the other hand, drastically reduces sensitivity below the full-observed reporting value.
It achieves a minimum sensitivity at five descriptors before reversing and linearly increases until the final sensitivity value.

\emph{Effect on specificity}
The selection feedback paths for specificity are nearly identical to the accuracy paths described above.
This is due to the large amount of benign cases which cause a large amount of false-positive errors.
Thus the specificity (equivalent to 1-false positive rate) is the main driver of accuracy.


\subsection{Ranking descriptor importance with selection criteria}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{desc_rank.pdf}
	\caption[Ranking descriptor importance with selection criteria]{Box plot of rank of observations requested using different selection criteria. Centerline is the median, the box represents quartiles, whiskers represent entire range, and outliers are points beyond whiskers .``Random'' selects the next observation randomly, ``Mutual Information'' selects the next observation based on maximal mutual information, and ``Optimal'' selects the next feature that will reduce cross-entropy loss.}
	\label{fig:feedback_mammo_ranks}
\end{figure}

Every selection criterion selects a different path of descriptors for every report.
Since selection criteria must select descriptors that most improve classification accuracy (besides random selection), the rank statistics of each descriptor provide a measure for its importance conditioned on prior observations.
Figure \ref{fig:feedback_mammo_ranks} shows box plots of the distributions of descriptor rankings.
The relative rankings of descriptors re-discover previously studied BI-RADS descriptor rankings \cite{Wu:2013bx}.


\emph{Random selection rankings}
Random selection does not provide any information about the descriptor ranks.
As a result, all have a median rank between 9 and 11, and their tails span the entire distribution of cases.
The box plot of these rankings is included as a reference point for selection criteria with no information about descriptor importance.

\emph{Mutual information selection rankings}
Mutual information will always select the same initial feature, BI-RADS assessment category, because it has the highest mutual information conditioned on no prior observations.
Stochasticity then emerges in the subsequent descriptors based upon report content.
Nonetheless, descriptors which are generally considered important still consistently rank highly.
Demographic information such as age, personal history of breast cancer, and family history of breast cancer are all within the top five descriptors.
The other category of highly ranked descriptors is mass characteristics including margin, density, and stability.
Lower ranked descriptors are infrequently used observations to describe what are generally referred to as associated findings to the main lesion.

\emph{Optimal selection rankings}
Optimal selection generally follows a similar early path as mutual information, selecting BI-RADS assessment category and demographic descriptors.
It deviates significantly after the top five descriptors by putting several associated findings in the top ten descriptors, as well as downplaying the importance of traditionally informative features such as breast density and mass margin.
Descriptors describing associated findings are infrequently used, but if they exist, provide strong evidence of presence or absence of malignancy.
Such sparse but impactful descriptors are more difficult to identify and report on.
This is in contrast to mass descriptors, which are readily available when a mass is in the image and known to be informative.

 
\subsection{Performance of (Selection, Stop) criteria pairs}

\begin{figure}[hb]
	\centering
	\includegraphics[width=\linewidth]{stop_histograms.pdf}
	\caption[Histograms of number of features selected for feedback]{Histograms depicting how many features a (selection,stop) criteria pair request. Each column is a pair, and each row is a threshold on the stop score. A good performing pair will request fewer features while still maintaining satisfactory performance metrics. Note: histograms where the 20 features are selected every time not plotted as ill conditioned for density estimate lines}
	\label{fig:feedback_stop_histograms}
\end{figure}

All stop criteria in this study are parametrized by a threshold and the selection criteria used to obtain new features.
Each of these groups of stop criteria, threshold, and selection criteria has its own number of features requested until stopping.
We record how many observations are requested and plot their histogram across all 10,000 reports in figure \ref{fig:feedback_stop_histograms}.
Thresholds are modulated along the rows, and selection/stop criteria pairs are modulated along columns.
Despite the large amount of feedback methods, there are three common patterns in these histograms which we describe as: all descriptors, all or none, minimal information.

\emph{All descriptors}:
Feedback methods that select all possible descriptors would place undo burden on radiologists, and in this context, are the worst performers.
There are two main drivers of methods that select all descriptors: random selection and low-threshold entropy stopping.

\emph{All or no descriptors}:
Histograms demonstrating this pattern either stop within 3 descriptors or request all 20 descriptors.
Examples of this behavior is low-threshold completeness stopping, and medium threshold ($t$ between 0.001 and 0.01) entropy.

\emph{Minimal information requested}:
Feedback methods that stop within few requested descriptors (under 5) are considered minimal information feedback criteria.
Completeness with medium to high threshold values (outside of random selection) show such behavior, though when coupled with mutual information selection, it still has non-trivial amount of descriptors reach 20 requests.
Entropy also requests minimal information when using high thresholds.
 

\subsubsection{Classification performance with selection, stop criteria}
\begin{table}[h]
	\centering
	\caption[Feedback system performance]{Table of all stop/selection feedback method results, depicting number of features selected (mean and median), accuracy (acc.), sensitivity (sens.), and specificity (spec.).Entries in red are the optimal values (minimum features or maximal classification statistics) without using optimal feedback. Entries in italics include optimal feedback. Numbers marked with a ``*'' indicate statistically significant difference from baseline classification with fully-observed report at p<0.05.}
	\label{table:feedback_perf}
	\begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|X|}
		\hline
		feedback criteria        & stop criteria            & threshold & mean features & median features  & acc.           & sens.                & spec.          \\ \hline
		\multirow{8}{*}{random}  & \multirow{4}{*}{comp.}   & 0         & 16.4          & 19               & 0.921          & \textcolor{red}{\textit{0.940}}      & 0.920          \\ \cline{3-8} 
		&                          & 0.001     & 12.8          & 13               & 0.923*          & 0.936                & 0.922*          \\ \cline{3-8} 
		&                          & 0.01      & 7.8           & 6                & 0.934*          & 0.843*                & 0.936*          \\ \cline{3-8} 
		&                          & 0.02      & 5.4           & 4                & 0.894*          & 0.739*                & 0.898*          \\ \cline{2-8} 
		& \multirow{4}{*}{entropy} & 0         & 20.0          & 20               & 0.921          & \textcolor{red}{\textit{0.940}}       & 0.920          \\ \cline{3-8} 
		&                          & 0.001     & 17.1          & 20               & 0.921          & 0.940                & 0.920          \\ \cline{3-8} 
		&                          & 0.01      & 13.8          & 14               & 0.921*          & 0.936                & 0.921*          \\ \cline{3-8} 
		&                          & 0.02      & 11.4          & 11               & 0.924*          & 0.932                & 0.923*          \\ \hline
		\multirow{8}{*}{mi}      & \multirow{4}{*}{comp.}   & 0         & 11.5          & 13               & 0.921          & \textcolor{red}{\textit{0.940}} & 0.920          \\ \cline{3-8} 
		&                          & 0.001     & 6.1           & 3                & 0.926          & 0.928                & 0.926          \\ \cline{3-8} 
		&                          & 0.01      & 4.0           & \textcolor{red}{\textit{2}} & \textcolor{red}{0.936}*    & 0.908                & \textcolor{red}{0.937}*    \\ \cline{3-8} 
		&                          & 0.02      & \textcolor{red}{3.6}     & \textcolor{red}{\textit{2}} & 0.931*          & 0.920                & 0.931*          \\ \cline{2-8} 
		& \multirow{4}{*}{entropy} & 0         & 20.0          & 20               & 0.921          & \textcolor{red}{\textit{0.940}} & 0.920          \\ \cline{3-8} 
		&                          & 0.001     & 12.9          & 20               & 0.921          & \textcolor{red}{\textit{0.940}} & 0.920          \\ \cline{3-8} 
		&                          & 0.01      & 8.4           & 3                & 0.921          & \textcolor{red}{\textit{0.940}} & 0.920          \\ \cline{3-8} 
		&                          & 0.02      & 5.3           & \textcolor{red}{\textit{2}} & 0.926          & 0.928                & 0.926          \\ \hline
		\multirow{8}{*}{optimal} & \multirow{4}{*}{comp.}   & 0         & 9.0           & 3                & 0.925*          & \textit{0.940}       & 0.924*          \\ \cline{3-8} 
		&                          & 0.001     & 4.3           & 3                & 0.955*          & \textit{0.940}       & 0.955*          \\ \cline{3-8} 
		&                          & 0.01      & 2.5           & \textit{2}       & 0.997*          & \textit{0.940}       & 0.998*          \\ \cline{3-8} 
		&                          & 0.02      & \textit{2.3}  & \textit{2}       & \textit{0.998}* & \textit{0.940}       & \textit{0.999}* \\ \cline{2-8} 
		& \multirow{4}{*}{entropy} & 0         & 20.0          & 20               & 0.921          & \textit{0.940}       & 0.920          \\ \cline{3-8} 
		&                          & 0.001     & 12.0          & 20               & 0.921          & \textit{0.940}       & 0.920          \\ \cline{3-8} 
		&                          & 0.01      & 5.4           & \textit{2}       & 0.929*          & \textit{0.940}       & 0.929*          \\ \cline{3-8} 
		&                          & 0.02      & 3.4           & \textit{2}       & 0.948*          & \textit{0.940}       & 0.949*          \\ \hline 
	\end{tabularx}
\end{table}

The overall performance of this system requires balancing the trade-off between selecting the minimal amount of descriptors while maximizing classification performance.
Table \ref{table:feedback_perf} shows the overall performance of all possible selection, and stop criteria pairs.
We record number of features requested (mean and median), classification accuracy, sensitivity, and specificity.
Number of features requested is considered better when the value is smaller since it means the report is faster to create.
Classification accuracy, sensitivity, and specificity are considered better when they are larger.
We highlight top performing feedback methods without using optimal selection in red, and overall top performing methods with italics.

We consider the baseline performance as the performance of the decision-support system which requests all features.
This results in a classification accuracy of 92.1\%, sensitivity of 94.0\%, and specificity of 92.0\%.
Overall, the best achieved accuracy was 99.8\% (optimal selection, completeness < 0.02 stopping), best sensitivity was 94.0\% (several cases), and best specificity was 99.9\% (optimal selection, completeness < 0.02 stopping).
Accuracy and specificity were statistically significantly better than baseline performance ($p\le6E-170$ for both cases).
No method had better sensitivity than baseline performance.
The lowest median descriptors requested was 2, and the lowest median descriptors requested was 2.34.

We also differentiate performance using optimal selection from the other selection methods because it is impossible to request optimal descriptors in practice.
Without using optimal feedback, the best accuracy was 93.6\% (mutual information selection, completeness < 0.01 stopping), best sensitivity 94.0\% (several), and best specificity 93.7\% (mutual information, completeness < 0.01 stopping).
Once again, accuracy and specificity were statistically significantly better than baseline ($p<2.5E-5$ and $4.6E-6$ respectively).
There was not significant difference between baseline and optimal sensitivity.
The lowest median descriptors selected was 2 and the lowest mean features selected was 3.6 (both achieved by mutual information, completeness but with different thresholds).







